{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":262177,"sourceType":"datasetVersion","datasetId":107946}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":105.23867,"end_time":"2025-07-02T18:17:26.010699","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-07-02T18:15:40.772029","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a2d11a79","cell_type":"markdown","source":"## Signature Verification using Siamese Networks","metadata":{"papermill":{"duration":0.00451,"end_time":"2025-07-02T18:15:44.839840","exception":false,"start_time":"2025-07-02T18:15:44.835330","status":"completed"},"tags":[]}},{"id":"a6ed1bb4","cell_type":"code","source":"import tensorflow as tf\nimport os","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-12-02T22:17:42.099097Z","iopub.execute_input":"2025-12-02T22:17:42.099398Z","iopub.status.idle":"2025-12-02T22:17:42.103021Z","shell.execute_reply.started":"2025-12-02T22:17:42.099374Z","shell.execute_reply":"2025-12-02T22:17:42.102371Z"},"papermill":{"duration":15.731834,"end_time":"2025-07-02T18:16:00.582118","exception":false,"start_time":"2025-07-02T18:15:44.850284","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":24},{"id":"d1149192","cell_type":"code","source":"path = '/kaggle/input/signature-verification-dataset/sign_data/train'\nls = os.listdir(path)\nprint('training data directories',len(ls))","metadata":{"execution":{"iopub.status.busy":"2025-12-02T22:17:42.104142Z","iopub.execute_input":"2025-12-02T22:17:42.104365Z","iopub.status.idle":"2025-12-02T22:17:42.119496Z","shell.execute_reply.started":"2025-12-02T22:17:42.104349Z","shell.execute_reply":"2025-12-02T22:17:42.118824Z"},"papermill":{"duration":0.031755,"end_time":"2025-07-02T18:16:00.617843","exception":false,"start_time":"2025-07-02T18:16:00.586088","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"training data directories 128\n","output_type":"stream"}],"execution_count":25},{"id":"d5fa2ba7","cell_type":"markdown","source":"#### Preprocess & Pair the signatures\n","metadata":{"papermill":{"duration":0.003485,"end_time":"2025-07-02T18:16:00.625254","exception":false,"start_time":"2025-07-02T18:16:00.621769","status":"completed"},"tags":[]}},{"id":"a85bf6f1","cell_type":"markdown","source":"input format: ( (img1, img2), label)  \nlabel = 1: Two genuine signatures from the same person.  \nlabel = 0: One genuine and one forged signature from the same person.  ","metadata":{"papermill":{"duration":0.003296,"end_time":"2025-07-02T18:16:00.632121","exception":false,"start_time":"2025-07-02T18:16:00.628825","status":"completed"},"tags":[]}},{"id":"f16fb149","cell_type":"code","source":"import random\nfrom PIL import Image\nimport numpy as np\n\ndef load_signature_pairs(data_dir):\n    pairs = []\n    labels = []\n    \n    users = sorted([name for name in os.listdir(data_dir) if '_' not in name])\n    \n    for user in users:\n        genuine_dir = os.path.join(data_dir, user)\n        forg_dir = genuine_dir + '_forg'\n        \n        genuine_imgs = os.listdir(genuine_dir)\n        forgery_imgs = os.listdir(forg_dir)\n        \n        num_genuine = len(genuine_imgs)\n        \n        # Random positive pairs (genuine vs different genuine)\n        random_indices = [\n            random.choice([j for j in range(num_genuine) if j != i])\n            for i in range(num_genuine)\n        ]\n        \n        for i in range(num_genuine):\n            img1 = os.path.join(genuine_dir, genuine_imgs[i])\n            img2 = os.path.join(genuine_dir, genuine_imgs[random_indices[i]])\n            pairs.append((img1, img2))\n            labels.append(1)\n\n        # Negative pairs (genuine vs forgery)\n        for i in range(min(len(genuine_imgs), len(forgery_imgs))):\n            img1 = os.path.join(genuine_dir, genuine_imgs[i])\n            img2 = os.path.join(forg_dir, forgery_imgs[i])\n            pairs.append((img1, img2))\n            labels.append(0)\n    \n    return pairs, labels\n","metadata":{"execution":{"iopub.status.busy":"2025-12-02T22:17:42.121370Z","iopub.execute_input":"2025-12-02T22:17:42.121598Z","iopub.status.idle":"2025-12-02T22:17:42.129500Z","shell.execute_reply.started":"2025-12-02T22:17:42.121579Z","shell.execute_reply":"2025-12-02T22:17:42.128816Z"},"papermill":{"duration":0.012374,"end_time":"2025-07-02T18:16:00.648013","exception":false,"start_time":"2025-07-02T18:16:00.635639","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":26},{"id":"370ec5bd","cell_type":"code","source":"path = '/kaggle/input/signature-verification-dataset/sign_data/train'\npairs, labels = load_signature_pairs(path)\nprint(len(pairs), len(labels))","metadata":{"execution":{"iopub.status.busy":"2025-12-02T22:17:42.160245Z","iopub.execute_input":"2025-12-02T22:17:42.160882Z","iopub.status.idle":"2025-12-02T22:17:42.309697Z","shell.execute_reply.started":"2025-12-02T22:17:42.160858Z","shell.execute_reply":"2025-12-02T22:17:42.309115Z"},"papermill":{"duration":1.774539,"end_time":"2025-07-02T18:16:02.426252","exception":false,"start_time":"2025-07-02T18:16:00.651713","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"1606 1606\n","output_type":"stream"}],"execution_count":27},{"id":"cf0cc3d1-6048-440a-b7b4-30eb9be5e376","cell_type":"code","source":"import os\n\nfor p1, p2 in pairs[:5]:\n    print(p1, os.path.exists(p1))\n    print(p2, os.path.exists(p2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T22:17:42.310526Z","iopub.execute_input":"2025-12-02T22:17:42.310798Z","iopub.status.idle":"2025-12-02T22:17:42.322297Z","shell.execute_reply.started":"2025-12-02T22:17:42.310772Z","shell.execute_reply":"2025-12-02T22:17:42.321600Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/signature-verification-dataset/sign_data/train/001/001_15.PNG True\n/kaggle/input/signature-verification-dataset/sign_data/train/001/001_01.PNG True\n/kaggle/input/signature-verification-dataset/sign_data/train/001/001_18.PNG True\n/kaggle/input/signature-verification-dataset/sign_data/train/001/001_05.PNG True\n/kaggle/input/signature-verification-dataset/sign_data/train/001/001_24.PNG True\n/kaggle/input/signature-verification-dataset/sign_data/train/001/001_22.PNG True\n/kaggle/input/signature-verification-dataset/sign_data/train/001/001_02.PNG True\n/kaggle/input/signature-verification-dataset/sign_data/train/001/001_15.PNG True\n/kaggle/input/signature-verification-dataset/sign_data/train/001/001_08.PNG True\n/kaggle/input/signature-verification-dataset/sign_data/train/001/001_05.PNG True\n","output_type":"stream"}],"execution_count":28},{"id":"a2726cef","cell_type":"markdown","source":"### backbone","metadata":{"papermill":{"duration":0.003675,"end_time":"2025-07-02T18:16:02.441500","exception":false,"start_time":"2025-07-02T18:16:02.437825","status":"completed"},"tags":[]}},{"id":"d81975fc","cell_type":"markdown","source":"caluculate embeddings for each image to compare similarity (how close they are in space)  \nInput: 224Ã—224 grayscale image  \nOutput: 256-dimensional embedding for input image.","metadata":{"papermill":{"duration":0.003573,"end_time":"2025-07-02T18:16:02.449034","exception":false,"start_time":"2025-07-02T18:16:02.445461","status":"completed"},"tags":[]}},{"id":"1030ee9e","cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\n\nIMG_SIZE = 224\n\n# ---------------------------\n# Custom Layer for L2 Normalization\n# ---------------------------\nclass L2Normalization(layers.Layer):\n    def call(self, inputs):\n        return tf.nn.l2_normalize(inputs, axis=1)\n\n# ---------------------------\n# Backbone Builder\n# ---------------------------\ndef build_backbone():\n    \"\"\"\n    Builds a CNN backbone .\n    \"\"\"\n    inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n    x = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n    x = layers.MaxPooling2D()(x)\n    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(256, activation='relu')(x)\n    x = L2Normalization()(x)  # Normalized embeddings\n    return Model(inputs, x, name=\"SimpleCNNBackbone\")\n","metadata":{"execution":{"iopub.status.busy":"2025-12-02T22:17:42.323093Z","iopub.execute_input":"2025-12-02T22:17:42.323326Z","iopub.status.idle":"2025-12-02T22:17:42.329289Z","shell.execute_reply.started":"2025-12-02T22:17:42.323306Z","shell.execute_reply":"2025-12-02T22:17:42.328591Z"},"papermill":{"duration":0.067505,"end_time":"2025-07-02T18:16:02.520400","exception":false,"start_time":"2025-07-02T18:16:02.452895","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":29},{"id":"fe124db2","cell_type":"markdown","source":"Feature Extraction:\nPass both images through the backbone (shared weights) to get embeddings.  \nSimilarity Calculation:\nUse L1 distance (absolute difference) between the two embeddings.  \nOutput Layer:\nsigmoid activation gives a similarity score (between 0 and 1).  \n","metadata":{"papermill":{"duration":0.003479,"end_time":"2025-07-02T18:16:02.527844","exception":false,"start_time":"2025-07-02T18:16:02.524365","status":"completed"},"tags":[]}},{"id":"0918f9d6","cell_type":"markdown","source":"### siamese neural network","metadata":{"papermill":{"duration":0.003351,"end_time":"2025-07-02T18:16:02.534705","exception":false,"start_time":"2025-07-02T18:16:02.531354","status":"completed"},"tags":[]}},{"id":"d276b123","cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\n\n# ---------------------------\n# Custom Layer for L1 Distance\n# ---------------------------\nclass L1Distance(layers.Layer):\n    \"\"\"Compute element-wise absolute difference between two embeddings.\"\"\"\n    def call(self, inputs):\n        return tf.abs(inputs[0] - inputs[1])\n\n# ---------------------------\n# Siamese Network Builder\n# ---------------------------\ndef build_siamese_network(backbone):\n    input_1 = layers.Input(shape=(224, 224, 1), name=\"image_1\")\n    input_2 = layers.Input(shape=(224, 224, 1), name=\"image_2\")\n\n    # Extract embeddings from backbone\n    embed_1 = backbone(input_1)\n    embed_2 = backbone(input_2)\n\n    # Compute L1 distance using custom layer\n    distance = L1Distance()([embed_1, embed_2])\n\n    # Final classification head\n    output = layers.Dense(1, activation='sigmoid')(distance)\n\n    # Build the model\n    model = Model(inputs=[input_1, input_2], outputs=output, name=\"SiameseNetwork\")\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2025-12-02T22:17:42.350843Z","iopub.execute_input":"2025-12-02T22:17:42.351039Z","iopub.status.idle":"2025-12-02T22:17:42.358019Z","shell.execute_reply.started":"2025-12-02T22:17:42.351024Z","shell.execute_reply":"2025-12-02T22:17:42.357086Z"},"papermill":{"duration":0.009986,"end_time":"2025-07-02T18:16:02.548685","exception":false,"start_time":"2025-07-02T18:16:02.538699","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":30},{"id":"66da5861","cell_type":"code","source":"backbone = build_backbone()\nsiamese_model = build_siamese_network(backbone)\nsiamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nsiamese_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2025-12-02T22:17:42.359096Z","iopub.execute_input":"2025-12-02T22:17:42.359262Z","iopub.status.idle":"2025-12-02T22:17:42.422981Z","shell.execute_reply.started":"2025-12-02T22:17:42.359249Z","shell.execute_reply":"2025-12-02T22:17:42.422435Z"},"papermill":{"duration":2.582074,"end_time":"2025-07-02T18:16:05.134404","exception":false,"start_time":"2025-07-02T18:16:02.552330","status":"completed"},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"SiameseNetwork\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"SiameseNetwork\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ image_1             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\nâ”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ \u001b[38;5;34m1\u001b[0m)                â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ image_2             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\nâ”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ \u001b[38;5;34m1\u001b[0m)                â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ SimpleCNNBackbone   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚    \u001b[38;5;34m107,520\u001b[0m â”‚ image_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    â”‚\nâ”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚                   â”‚            â”‚ image_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ l1_distance_1       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ SimpleCNNBackbonâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mL1Distance\u001b[0m)        â”‚                   â”‚            â”‚ SimpleCNNBackbonâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         â”‚        \u001b[38;5;34m257\u001b[0m â”‚ l1_distance_1[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ image_1             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ image_2             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ SimpleCNNBackbone   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">107,520</span> â”‚ image_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚                   â”‚            â”‚ image_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ l1_distance_1       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ SimpleCNNBackbonâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">L1Distance</span>)        â”‚                   â”‚            â”‚ SimpleCNNBackbonâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> â”‚ l1_distance_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m107,777\u001b[0m (421.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">107,777</span> (421.00 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m107,777\u001b[0m (421.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">107,777</span> (421.00 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":31},{"id":"609a10c8","cell_type":"markdown","source":"input image -> grayscale, [224, 224] px, normalized","metadata":{"papermill":{"duration":0.003924,"end_time":"2025-07-02T18:16:05.142824","exception":false,"start_time":"2025-07-02T18:16:05.138900","status":"completed"},"tags":[]}},{"id":"f8c83c2b","cell_type":"code","source":"def preprocess_image(path):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_png(image, channels=1)  # grayscale\n    image = tf.image.resize(image, [224, 224])\n    image = tf.cast(image, tf.float32) / 255.0\n    return image  # shape: (224, 224, 1)","metadata":{"execution":{"iopub.status.busy":"2025-12-02T22:17:42.423641Z","iopub.execute_input":"2025-12-02T22:17:42.423838Z","iopub.status.idle":"2025-12-02T22:17:42.427647Z","shell.execute_reply.started":"2025-12-02T22:17:42.423823Z","shell.execute_reply":"2025-12-02T22:17:42.427050Z"},"papermill":{"duration":0.010061,"end_time":"2025-07-02T18:16:05.156959","exception":false,"start_time":"2025-07-02T18:16:05.146898","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":32},{"id":"1a3fd854","cell_type":"code","source":"def make_tf_dataset(pairs, labels, batch_size=32, shuffle=True):\n    path_ds = tf.data.Dataset.from_tensor_slices((pairs, labels))\n\n    def load_images(pair, label):\n        img1 = preprocess_image(pair[0])\n        img2 = preprocess_image(pair[1])\n        return (img1, img2), label\n\n    dataset = path_ds.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)\n    # num_parallel_calls=tf.data.AUTOTUNE lets TensorFlow load multiple images in parallel = faster.\n    dataset = dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2025-12-02T22:17:42.428355Z","iopub.execute_input":"2025-12-02T22:17:42.428821Z","iopub.status.idle":"2025-12-02T22:17:42.441170Z","shell.execute_reply.started":"2025-12-02T22:17:42.428805Z","shell.execute_reply":"2025-12-02T22:17:42.440517Z"},"papermill":{"duration":0.009889,"end_time":"2025-07-02T18:16:05.171116","exception":false,"start_time":"2025-07-02T18:16:05.161227","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":33},{"id":"034ce855","cell_type":"code","source":"train_dataset = make_tf_dataset(pairs, labels, batch_size=32)\nprint(len(train_dataset))","metadata":{"execution":{"iopub.status.busy":"2025-12-02T22:17:42.443088Z","iopub.execute_input":"2025-12-02T22:17:42.443296Z","iopub.status.idle":"2025-12-02T22:17:42.487403Z","shell.execute_reply.started":"2025-12-02T22:17:42.443281Z","shell.execute_reply":"2025-12-02T22:17:42.486864Z"},"papermill":{"duration":0.109983,"end_time":"2025-07-02T18:16:05.285108","exception":false,"start_time":"2025-07-02T18:16:05.175125","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"51\n","output_type":"stream"}],"execution_count":34},{"id":"d0ccbae8","cell_type":"code","source":"siamese_model.fit(train_dataset, epochs=10)","metadata":{"execution":{"iopub.status.busy":"2025-12-02T22:17:42.488065Z","iopub.execute_input":"2025-12-02T22:17:42.488251Z","iopub.status.idle":"2025-12-02T22:18:46.149377Z","shell.execute_reply.started":"2025-12-02T22:17:42.488236Z","shell.execute_reply":"2025-12-02T22:18:46.148818Z"},"papermill":{"duration":70.67178,"end_time":"2025-07-02T18:17:15.960993","exception":false,"start_time":"2025-07-02T18:16:05.289213","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m51/51\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 89ms/step - accuracy: 0.6851 - loss: 0.6775\nEpoch 2/10\n\u001b[1m51/51\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - accuracy: 0.8537 - loss: 0.5747\nEpoch 3/10\n\u001b[1m51/51\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - accuracy: 0.8834 - loss: 0.5235\nEpoch 4/10\n\u001b[1m51/51\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - accuracy: 0.9062 - loss: 0.4985\nEpoch 5/10\n\u001b[1m51/51\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - accuracy: 0.8961 - loss: 0.4617\nEpoch 6/10\n\u001b[1m51/51\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - accuracy: 0.9111 - loss: 0.4672\nEpoch 7/10\n\u001b[1m51/51\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - accuracy: 0.9145 - loss: 0.4644\nEpoch 8/10\n\u001b[1m51/51\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - accuracy: 0.9041 - loss: 0.4481\nEpoch 9/10\n\u001b[1m51/51\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - accuracy: 0.9211 - loss: 0.4045\nEpoch 10/10\n\u001b[1m51/51\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - accuracy: 0.9239 - loss: 0.3957\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7ee2cde86710>"},"metadata":{}}],"execution_count":35},{"id":"7fc5890d","cell_type":"code","source":"# Load test data\npath = '/kaggle/input/signature-verification-dataset/sign_data/test'\npairs, labels = load_signature_pairs(path)\n\nprint(\"Total samples:\", len(pairs))\n\ntest_dataset = make_tf_dataset(pairs, labels, batch_size=32)\nprint(\"Batches:\", tf.data.experimental.cardinality(test_dataset).numpy())\n\n# Evaluate model\nloss, accuracy = siamese_model.evaluate(test_dataset)\nprint(f\"\\nTest Accuracy: {accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2025-12-02T22:18:46.150211Z","iopub.execute_input":"2025-12-02T22:18:46.150542Z","iopub.status.idle":"2025-12-02T22:18:48.414806Z","shell.execute_reply.started":"2025-12-02T22:18:46.150518Z","shell.execute_reply":"2025-12-02T22:18:48.414115Z"},"papermill":{"duration":5.86486,"end_time":"2025-07-02T18:17:21.851109","exception":false,"start_time":"2025-07-02T18:17:15.986249","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Total samples: 476\nBatches: 15\n\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9775 - loss: 0.3317\n\nTest Accuracy: 0.9748\n","output_type":"stream"}],"execution_count":36},{"id":"540439eb-6991-4d0e-8ab4-81b8a8b0d639","cell_type":"code","source":"print(test_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T22:18:48.415538Z","iopub.execute_input":"2025-12-02T22:18:48.415816Z","iopub.status.idle":"2025-12-02T22:18:48.420003Z","shell.execute_reply.started":"2025-12-02T22:18:48.415797Z","shell.execute_reply":"2025-12-02T22:18:48.419278Z"}},"outputs":[{"name":"stdout","text":"<_PrefetchDataset element_spec=((TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name=None)), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n","output_type":"stream"}],"execution_count":37},{"id":"bc0b9629","cell_type":"code","source":"img_path1 = '/kaggle/input/signature-verification-dataset/sign_data/train/017_forg/01_0107017.PNG'\nimg_path2 = '/kaggle/input/signature-verification-dataset/sign_data/train/017/01_017.png'\ndef predict_similarity(model, img_path1, img_path2):\n    img1 = preprocess_image(img_path1)\n    img2 = preprocess_image(img_path2)\n\n    # Add batch dimension: (1, 224, 224, 1)\n    img1 = tf.expand_dims(img1, axis=0)\n    img2 = tf.expand_dims(img2, axis=0)\n\n    prediction = model.predict([img1, img2])[0][0]  # sigmoid output\n\n    print(f\"Similarity Score: {prediction:.4f}\")\n    if prediction >= 0.55:\n        print(\"Genuine\")\n    else:\n        print(\"Forged \")","metadata":{"execution":{"iopub.status.busy":"2025-12-02T22:18:48.420951Z","iopub.execute_input":"2025-12-02T22:18:48.421206Z","iopub.status.idle":"2025-12-02T22:18:48.435499Z","shell.execute_reply.started":"2025-12-02T22:18:48.421183Z","shell.execute_reply":"2025-12-02T22:18:48.434795Z"},"papermill":{"duration":0.030791,"end_time":"2025-07-02T18:17:21.907223","exception":false,"start_time":"2025-07-02T18:17:21.876432","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":38},{"id":"02fd3e2c","cell_type":"code","source":"predict_similarity(siamese_model, img_path1, img_path2)","metadata":{"execution":{"iopub.status.busy":"2025-12-02T22:18:48.436851Z","iopub.execute_input":"2025-12-02T22:18:48.437081Z","iopub.status.idle":"2025-12-02T22:18:48.882723Z","shell.execute_reply.started":"2025-12-02T22:18:48.437067Z","shell.execute_reply":"2025-12-02T22:18:48.881880Z"},"papermill":{"duration":0.610901,"end_time":"2025-07-02T18:17:22.542968","exception":false,"start_time":"2025-07-02T18:17:21.932067","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step\nSimilarity Score: 0.0414\nForged \n","output_type":"stream"}],"execution_count":39},{"id":"9fe881d4","cell_type":"markdown","source":"save the model","metadata":{"papermill":{"duration":0.07686,"end_time":"2025-07-02T18:17:22.645287","exception":false,"start_time":"2025-07-02T18:17:22.568427","status":"completed"},"tags":[]}},{"id":"3bd05a99","cell_type":"code","source":"siamese_model.save(\"Siamese_signature.keras\")\n","metadata":{"execution":{"iopub.status.busy":"2025-12-02T22:33:07.847007Z","iopub.execute_input":"2025-12-02T22:33:07.847566Z","iopub.status.idle":"2025-12-02T22:33:07.899347Z","shell.execute_reply.started":"2025-12-02T22:33:07.847520Z","shell.execute_reply":"2025-12-02T22:33:07.898498Z"},"papermill":{"duration":0.089829,"end_time":"2025-07-02T18:17:22.759982","exception":false,"start_time":"2025-07-02T18:17:22.670153","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":45},{"id":"804a87b4-1257-4158-bfb6-b90862048c28","cell_type":"code","source":"import tensorflow as tf\n\nsiamese_model = tf.keras.models.load_model(\n    \"Siamese_signature.keras\",\n    custom_objects={\"L1Distance\": L1Distance, \"L2Normalization\": L2Normalization}\n)\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(siamese_model)\ntflite_model = converter.convert()\n\n# Ø­ÙØ¸Ù‡\nwith open(\"Siamese_signature.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T22:34:12.448252Z","iopub.execute_input":"2025-12-02T22:34:12.448577Z","iopub.status.idle":"2025-12-02T22:34:13.331426Z","shell.execute_reply.started":"2025-12-02T22:34:12.448537Z","shell.execute_reply":"2025-12-02T22:34:13.330722Z"}},"outputs":[{"name":"stdout","text":"INFO:tensorflow:Assets written to: /tmp/tmpgmkmzezo/assets\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Assets written to: /tmp/tmpgmkmzezo/assets\n","output_type":"stream"},{"name":"stdout","text":"Saved artifact at '/tmp/tmpgmkmzezo'. The following endpoints are available:\n\n* Endpoint 'serve'\n  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name='image_1'), TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name='image_2')]\nOutput Type:\n  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\nCaptures:\n  139512553729168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139512553728592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139512553729936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139512553727248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139512553728400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139512553726672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139512553724752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139512553730128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1764714853.189159     322 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\nW0000 00:00:1764714853.189185     322 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n","output_type":"stream"}],"execution_count":47},{"id":"fc9f279d","cell_type":"code","source":"# Import necessary libraries\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\nimport numpy as np\nimport gradio as gr\nfrom PIL import Image\nimport os\nimport sys\n\n# Image size used during model training (constant)\nIMG_SIZE = 224\n\n# Verification threshold (55%): Defines the minimum required similarity for a match\nVERIFICATION_THRESHOLD = 0.55\n\n# 1. Custom Layer Definitions (Mandatory for model loading)\n# These definitions are necessary because the trained model uses these custom layers.\nclass L2Normalization(layers.Layer):\n    \"\"\"Applies L2 normalization to the input vector.\"\"\"\n    def call(self, inputs):\n        return tf.nn.l2_normalize(inputs, axis=1)\n\nclass L1Distance(layers.Layer):\n    \"\"\"Calculates the absolute difference between two vectors.\"\"\"\n    def call(self, inputs):\n        return tf.abs(inputs[0] - inputs[1])\n\n# Required list for loading the model with custom layers\nCUSTOM_OBJECTS = {\n    \"L2Normalization\": L2Normalization,\n    \"L1Distance\": L1Distance\n}\n\n# 2. Model Path Definition\nMODEL_PATH = '/kaggle/working/Siamese_signature.keras'\n\n# 3. Fallback Model Building Function (Used if the trained file is not found)\ndef build_fallback_model():\n    \"\"\"Builds a simple Siamese model with random weights.\"\"\"\n    inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 1), name=\"input_image\")\n    # Simple CNN backbone structure\n    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(inputs)\n    x = layers.MaxPooling2D(pool_size=(2,2))(x)\n    x = layers.GlobalAveragePooling2D()(x)\n    embed = layers.Dense(256, activation='relu')(x)\n    embed = L2Normalization()(embed)\n    \n    input_2 = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 1), name=\"image_2\")\n    \n    # Create the backbone model for re-use\n    backbone_model = Model(inputs, embed)\n    \n    embed_1 = backbone_model(inputs)\n    embed_2 = backbone_model(input_2)\n    \n    distance = L1Distance()([embed_1, embed_2])\n    output = layers.Dense(1, activation='sigmoid')(distance)\n\n    model = Model(inputs=[inputs, input_2], outputs=output, name=\"SiameseFallback\")\n    model.compile(optimizer='adam', loss='binary_crossentropy')\n    return model\n\n# 4. Model Loading\ntry:\n    # Attempt to load the trained model\n    siamese_model = tf.keras.models.load_model(MODEL_PATH, custom_objects=CUSTOM_OBJECTS)\n    print(f\"âœ… Model loaded successfully from: {MODEL_PATH}. Ready for prediction.\")\nexcept Exception as e:\n    # Fallback plan if the file is not found\n    print(f\"âŒ Failed to load model from {MODEL_PATH}. Please ensure the saved file is in the notebook path.\")\n    print(f\"Building a fallback model with random weights instead of stopping. Error: {e}\")\n    siamese_model = build_fallback_model()\n    # Exit to prevent use of the untrained model if the trained one is mandatory\n    sys.exit(\"Stopped: Trained model not found. Please ensure the file path is correct.\")\n\n\n# 5. Image Pre-processing Function\ndef preprocess_image(img_arr):\n    \"\"\"\n    Processes the image: converts to grayscale, resizes, and normalizes to fit the model input.\n    \"\"\"\n    if img_arr is None:\n        return None\n    \n    # Convert to grayscale if not already\n    if img_arr.ndim == 3 and img_arr.shape[-1] == 3:\n        img = Image.fromarray(img_arr).convert('L')\n        img_arr = np.array(img)\n    elif img_arr.ndim == 2:\n        pass\n    else:\n        try:\n            img = Image.fromarray(img_arr).convert('L')\n            img_arr = np.array(img)\n        except:\n            return None\n        \n    # Resize\n    img = tf.image.resize(img_arr[..., np.newaxis], [IMG_SIZE, IMG_SIZE])\n    \n    # Normalization\n    img = tf.cast(img, tf.float32) / 255.0\n    \n    # Add batch dimension\n    return tf.expand_dims(img, axis=0)\n\n# 6. Gradio Prediction Function\ndef predict_match(image1, image2):\n    \"\"\"\n    Takes two images, processes them, and uses the model for prediction.\n    \"\"\"\n    img1_processed = preprocess_image(image1)\n    img2_processed = preprocess_image(image2)\n\n    if img1_processed is None or img2_processed is None:\n        # Error message for the user in English\n        return gr.Label(\"Please upload a valid image for both inputs for comparison.\", color=\"yellow\", label=\"Verification Result\")\n\n    # Perform prediction using the trained model\n    try:\n        # Prediction requires a list of inputs: [image1, image2]\n        prediction = siamese_model.predict([img1_processed, img2_processed], verbose=0)[0][0]\n    except Exception as e:\n        # Error message for the user in English\n        return gr.Label(f\"An error occurred during prediction. Error: {e}\", color=\"red\", label=\"Prediction Error\")\n    \n    # Interpret the result and compare against the threshold (0.55)\n    similarity_percentage = prediction * 100\n    \n    # Check for match based on the threshold\n    if prediction >= VERIFICATION_THRESHOLD:\n        match_status = \"Genuine âœ… (Match)\"\n        color = \"green\"\n    else:\n        match_status = \"Forged âŒ (No Match)\"\n        color = \"red\"\n        \n    output_text = match_status\n    \n    return gr.Label(output_text, color=color, label=\"Verification Result\")\n\n# 7. Create Gradio Interface\niface = gr.Interface(\n    fn=predict_match,\n    inputs=[\n        # Input labels in English\n        gr.Image(label=\"(Image 1)\", type=\"numpy\"),\n        gr.Image(label=\"(Image 2)\", type=\"numpy\")\n    ],\n    # Output label in English\n    outputs=gr.Label(label=\"Verification Result\"),\n    # Title and description in English\n    title=\"Signature Verification âœï¸ğŸ–‹ï¸\",\n    description=f\"Upload two signatures to classify as Genuine âœ… or Forged âŒ.\",\n    allow_flagging=\"never\",\n    # Interface theme\n    theme=gr.themes.Soft()\n)\n\n# 8. Launch the Application\niface.launch(share=True)\n","metadata":{"papermill":{"duration":0.024725,"end_time":"2025-07-02T18:17:22.864783","exception":false,"start_time":"2025-07-02T18:17:22.840058","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T22:46:41.887115Z","iopub.execute_input":"2025-12-02T22:46:41.887893Z","iopub.status.idle":"2025-12-02T22:46:44.588538Z","shell.execute_reply.started":"2025-12-02T22:46:41.887861Z","shell.execute_reply":"2025-12-02T22:46:44.587974Z"}},"outputs":[{"name":"stdout","text":"âœ… Model loaded successfully from: /kaggle/working/Siamese_signature.keras. Ready for prediction.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/gradio/interface.py:425: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7863\n* Running on public URL: https://1ec774457269333e0f.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://1ec774457269333e0f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":52}]}